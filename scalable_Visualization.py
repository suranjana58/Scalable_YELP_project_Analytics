# -*- coding: utf-8 -*-
"""Scalable_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t20pydWSVMxqFme2_A2paQH0mXyioyHS
"""

pip install pyspark

from pyspark.context import SparkContext
from pyspark.sql.session import SparkSession
from pyspark import SparkConf
from pyspark.sql.functions import count

#sc.stop()
conf = SparkConf().setMaster("local[*]").set("spark.executer.memory", "4g")

sc = SparkContext(conf=conf)
spark = SparkSession(sc).builder.getOrCreate()
spark.sparkContext.setLogLevel("ERROR")

# Imprting libraries needed for analysis
from pyspark.sql.functions import regexp_replace
import matplotlib.pyplot as plt
from pyspark.sql.functions import when
import re


#from pyspark.ml.feature import Tokenizer, HashingTF, IDF 
#from pyspark.ml.classification import LogisticRegression
#from pyspark.ml import Pipeline
from pyspark.sql.functions import col

import pandas as pd
import numpy as np

#df_business = spark.read.format("csv").option("header", "true").option("multiline","true").load("/content/drive/MyDrive/yelp_business.csv")
#df_business.show()

from pyspark.sql import SparkSession
from pyspark.sql.functions import col
import csv
try:
    with open("/content/drive/MyDrive/yelp_business.csv", "r") as file:
        reader = csv.reader(file)
        header = next(reader) # extract the header row
        data = [row for row in reader]
    df_business = spark.createDataFrame(data, schema=header)
    
    # Do some processing on the dataframe
    #df_filtered = df_business_hr.filter(col("some_column") == "some_value")
    # Do some more processing on the filtered dataframe
except csv.Error as e:
    print("An error occurred while parsing the CSV file: {}".format(e))
    # Handle the error and move on, for example:
    df_business = spark.createDataFrame([], df_business.schema) # create an empty dataframe with the same schema as df_business_hr
except Exception as e:
    print("An error occurred while processing the data: {}".format(e))
    # Handle the error and move on, for example:
    df_business = spark.createDataFrame([], df_business.schema) # create an empty dataframe with the same schema as df_business_hr

df_business.show()

df_business_attr = spark.read.format("csv").option("header", "true").option("multiline","true").load("/content/drive/MyDrive/yelp_business_attributes.csv")
df_business_attr.show(5)

from pyspark.sql import SparkSession
from pyspark.sql.functions import col
import csv
try:
    with open("/content/drive/MyDrive/yelp_business_hours.csv", "r") as file:
        reader = csv.reader(file)
        header = next(reader) # extract the header row
        data = [row for row in reader]
    df_business_hr = spark.createDataFrame(data, schema=header)
    
    # Do some processing on the dataframe
    #df_filtered = df_business_hr.filter(col("some_column") == "some_value")
    # Do some more processing on the filtered dataframe
except csv.Error as e:
    print("An error occurred while parsing the CSV file: {}".format(e))
    # Handle the error and move on, for example:
    df_business_hr = spark.createDataFrame([], df_business_hr.schema) # create an empty dataframe with the same schema as df_business_hr
except Exception as e:
    print("An error occurred while processing the data: {}".format(e))
    # Handle the error and move on, for example:
    df_business_hr = spark.createDataFrame([], df_business_hr.schema) # create an empty dataframe with the same schema as df_business_hr

df_business_hr.show()

df_checkin = spark.read.format("csv").option("header", "true").option("multiline","true").load("/content/drive/MyDrive/yelp_checkin.csv")
df_checkin.show()

#"review_id","user_id","business_id","stars","date","text","useful","funny","cool"
#1. Clean the dataset
df_review = spark.read.format("csv").option("header", "true").option("multiline","true").load("/content/drive/MyDrive/yelp_review.csv")

df_review.show()

df_user = spark.read.format("csv").option("header", "true").option("multiline","true").load("/content/drive/MyDrive/yelp_user.csv")

df_user.show()

df_business_attr = spark.read.format("csv").option("header", "true").option("multiline","true").load("/content/drive/MyDrive/yelp_business_attributes.csv")

df_business_attr.show()

df_tip = spark.read.format("csv").option("header", "true").option("multiline","true").load("/content/drive/MyDrive/yelp_tip.csv")

df_tip.show()

# Let us print schema for all
df_review.printSchema()

# Creating another column named 'label'. in which starts column have been converted to numeric
df_review = df_review.withColumn("stars", regexp_replace("stars", "[^0-9.]+", ""))
df_review = df_review.withColumn("label", df_review["stars"].cast("double"))
df_review.show()

# n_rows = df_review.count()
# n_cols = len(df_review.columns)

"""# Data Exploratory Analysis - REVIEWS

"""

#####  1: BAR CHART FOR NUMBER OF STARS ######
count_1_star = df_review.filter(df_review.label == 1.0).count()
count_2_star = df_review.filter(df_review.label == 2.0).count()
count_3_star = df_review.filter(df_review.label == 3.0).count()
count_4_star = df_review.filter(df_review.label == 4.0).count()
count_5_star = df_review.filter(df_review.label == 5.0).count()

print("Count of 1-star ratings:", count_1_star)
print("Count of 2-star ratings:", count_2_star)
print("Count of 3-star ratings:", count_3_star)
print("Count of 4-star ratings:", count_4_star)
print("Count of 5-star ratings:", count_5_star)

# Create a bar plot
labels = ['1-star', '2-star', '3-star', '4-star', '5-star']
counts = [count_1_star, count_2_star, count_3_star, count_4_star, count_5_star]
colors = ['#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']

ax = plt.axes()
ax.bar(labels, counts, color=colors)

# Add labels to the bars
for i, count in enumerate(counts):
    ax.text(i, count+1, str(count), ha='center', va='bottom')

# Add labels and title
plt.xlabel('Rating')
plt.ylabel('Count')
plt.title('Ratings Bar Chart')

# Display the plot
plt.show()

#####  2: PIE CHART FOR SENTIMENT OF REVIEWS ######
df_review = df_review.withColumn("sentiment", when(df_review["stars"] >= 4, "positive")
                                              .when(df_review["stars"] <= 2, "negative")
                                              .otherwise("neutral"))

count_positive = df_review.filter(df_review.sentiment == "positive").count()
count_negative = df_review.filter(df_review.sentiment == "negative").count()
count_neutral = df_review.filter(df_review.sentiment == "neutral").count()

print("Count of negative reviews:", count_negative)
print("Count of positive reviews:", count_positive)
print("Count of neutral reviews:", count_neutral)

# Define the values
values = [count_positive, count_negative, count_neutral]
labels = ['Positive Reviews', 'Negative Reviews', 'Neutral Reviews']

# Define the colors
colors = ['lightblue', 'violet', 'orange']

# Create the pie chart
plt.pie(values, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)

# Add a title
plt.title('Sentiment Analysis of Customer Reviews')

# Show the chart
plt.show()

#####  3: Dsitribution of ratings amongst different stars ######
df_review = df_review.withColumn('useful', regexp_replace('useful', '[^0-9]', ''))
df_review = df_review.withColumn('cool', regexp_replace('cool', '[^0-9]', ''))
df_review = df_review.withColumn('funny', regexp_replace('funny', '[^0-9]', ''))
#df_review = df_review.withColumn("useful", df_review["useful"].cast("single"))
#df_review = df_review.withColumn("cool", df_review["cool"].cast("single"))
#df_review = df_review.withColumn("funny", df_review["funny"].cast("single"))
df_review.show()


# Select the columns and group by them to count the number of rows in each group
# grouped = df_review.select('useful', 'funny', 'cool').groupBy('useful', 'funny', 'cool').count().limit(100)

# grouped.show()

df_review.show()

import pyspark.sql.functions as F
import matplotlib.pyplot as plt

# Aggregate the data by sentiment category and count the number of useful, cool, and funny
agg_df = df_review.groupBy('sentiment').agg(
    F.sum('useful').alias('useful_count'),
    F.sum('cool').alias('cool_count'),
    F.sum('funny').alias('funny_count'),
    F.count('review_id').alias('total_count')
)

# Extract the data from the aggregated DataFrame
sentiments = [row['sentiment'] for row in agg_df.collect()]
useful_counts = [row['useful_count'] for row in agg_df.collect()]
cool_counts = [row['cool_count'] for row in agg_df.collect()]
funny_counts = [row['funny_count'] for row in agg_df.collect()]
total_count = funny_counts = [row['total_count'] for row in agg_df.collect()]

agg_df.show()

# Create a bar plot
fig, ax = plt.subplots()
ax.bar(sentiments, useful_counts, label='Useful')
ax.bar(sentiments, cool_counts, bottom=useful_counts, label='Cool')
ax.bar(sentiments, funny_counts, bottom=[sum(x) for x in zip(useful_counts, cool_counts)], label='Funny')
ax.set_xlabel('Sentiment Category')
ax.set_ylabel('Count')
ax.set_title('Useful, Cool, and Funny Counts by Sentiment Category')
ax.legend()
#ax.set_yticks([0, 50, 100, 150, 200])
# Add labels to the bars
#for i, v in enumerate(useful_counts):
#    ax.text(i - 0.2, v + 5, str(v))
#for i, v in enumerate(cool_counts):
#    ax.text(i - 0.2, v + useful_counts[i] + 5, str(v))
#for i, v in enumerate(funny_counts):
#    ax.text(i - 0.2, v + useful_counts[i] + cool_counts[i] + 5, str(v))

# Show the plot
plt.show()
# Show the plot
plt.show()

# Define the data
labels = ['Useful', 'Cool', 'Funny']
sizes_positive = [3.009058076473183, 4.120027511918469, 4.385420420450390]
sizes_neutral = [3.4984541020234, 4.146147125150113, 2.1951130426954]
sizes_neutral = np.nan_to_num(sizes_neutral)
sizes_negative = [1.20097163004051, 4.23733610744847, 2.03417265715989]
sizes_negative = np.nan_to_num(sizes_negative)

# Create the pie chart for positive
fig, ax = plt.subplots()
colors = ['lightblue', 'yellow', 'indigo']
ax.pie(sizes_positive, colors=colors, labels=labels, autopct='%1.1f%%', startangle=90)
ax.set_title('Distribution of Useful, Cool, and Funny for Positive Sentiment')

# Show the plot
plt.show()

# Create the pie chart for Neutral
fig, ax = plt.subplots()
colors = ['lightblue', 'seagreen', 'orange']
ax.pie(sizes_neutral, colors=colors, labels=labels, autopct='%1.1f%%', startangle=90)
ax.set_title('Distribution of Useful, Cool, and Funny for Neutral Sentiment')

# Show the plot
plt.show()

# Create the pie chart for Negative
fig, ax = plt.subplots()
colors = ['lightgreen', 'silver', 'pink']
ax.pie(sizes_negative, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
ax.set_title('Distribution of Useful, Cool, and Funny for Negative Sentiment')

# Show the plot
plt.show()

"""### Data Exploratory Analysis - Business"""

######  1: BAR CHART FOR NUMBER OF STARS (Business) ######

# Creating another column named 'label'. in which starts column have been converted to numeric
df_business = df_business.withColumn("stars", regexp_replace("stars", "[^0-9.]+", ""))
df_business = df_business.withColumn("label", df_business["stars"].cast("double"))
#df_business.show()

df_business = df_business.withColumn("StarBucket", when(df_business["stars"] > 4, "5 star")
                                              .when(df_business["stars"] >3 , "4 star")
                                              .when(df_business["stars"] >2 , "3 star")
                                              .when(df_business["stars"] >1 , "2 star")
                                              .otherwise("1 star"))

count_1_star = df_business.filter(df_business.StarBucket == "1 star").count()
count_2_star = df_business.filter(df_business.StarBucket == "2 star").count()
count_3_star = df_business.filter(df_business.StarBucket == "3 star").count()
count_4_star = df_business.filter(df_business.StarBucket == "4 star").count()
count_5_star = df_business.filter(df_business.StarBucket == "5 star").count()



print("Count of 1-star ratings:", count_1_star)
print("Count of 2-star ratings:", count_2_star)
print("Count of 3-star ratings:", count_3_star)
print("Count of 4-star ratings:", count_4_star)
print("Count of 5-star ratings:", count_5_star)

# Create a bar plot
labels = ['0-1', '1-2', '2-3', '3-4', '4-5']
counts = [count_1_star, count_2_star, count_3_star, count_4_star, count_5_star]
colors = ['#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']

ax = plt.axes()
ax.bar(labels, counts, color=colors)

# Add labels to the bars
for i, count in enumerate(counts):
    ax.text(i, count+1, str(count), ha='center', va='bottom')

# Add some labels and a title
plt.xlabel('Rating')
plt.ylabel('Count')
plt.title('Ratings Bar Chart')

# Display the plot
plt.show()

import pyspark.sql.functions as F
from pyspark.sql.functions import sum as spark_sum
from pyspark.sql.window import Window


# assume your dataframe is called 'df'
stars_per_state = df_business.groupBy('state', 'StarBucket').count().orderBy('state', 'StarBucket')

#stars_per_state = df_business.groupBy('state', 'StarBucket').agg(spark_sum('count').alias('total_count'))
stars_per_state = stars_per_state.withColumn('percentage', stars_per_state['count'] / spark_sum('count').over(Window.partitionBy('state')) * 100)
stars_per_state.show()

# filter the dataframe to remove rows with certain state values
states_to_remove = ['01', '3', '30', '6', 'AB', 'ABE']
stars_per_state_filtered = stars_per_state.filter(~stars_per_state['state'].isin(states_to_remove))

# convert to Pandas dataframe for plotting
stars_per_state_filtered_pd = stars_per_state_filtered.toPandas()
stars_per_state_filtered.show()

# convert to Pandas dataframe for plotting
# stars_per_state_pd = stars_per_state.toPandas()

fig, ax = plt.subplots(figsize=(15, 6))

# plot the data using a bar chart
stars_per_state_filtered_pd.pivot(index='state', columns='StarBucket', values='count').plot(kind='bar', stacked=True, ax=ax)

# add labels to the bars
#for p in ax.containers:
#    for q in p.patches:
#        if q.get_height() > 0:
#            ax.annotate(str(int(q.get_height())), xy=(q.get_x() + q.get_width() / 2, q.get_y() + q.get_height() / 2),
#                        xytext=(0, 0), textcoords='offset points', ha='center', va='center')


# show the plot
plt.show()

"""### DATA EXPLORATORY ANALYSIS - REVIEWS + BUSINESSES"""

merged_df = df_review.join(df_business, df_review.business_id == df_business.business_id, "inner")

merged_df.show()

# Get the row count
row_count = merged_df.count()

# Get the column count
column_count = len(merged_df.columns)

# Print the row and column count
print("Row count: ", row_count)
print("Column count: ", column_count)

"""### DATA EXPLORATORY ANALYSIS - USERS

"""

df_user.show()
# Get the row count
row_count = df_user.count()

# Get the column count
column_count = len(df_user.columns)

# Print the row and column count
print("Row count: ", row_count)
print("Column count: ", column_count)

# Check for duplicates based on specific columns
duplicates = df_user.dropDuplicates(["user_id"])

# Count the number of duplicates
num_duplicates = df_user.count() - duplicates.count()

# Print the number of duplicates
print("Number of duplicates: ", num_duplicates)

# Elite vs Non Elite users
count_non_elite = df_user.filter(df_user.elite == "None").count()
count_elite = df_user.filter(df_user.elite != "None").count()

print("Count of non elite users:", count_non_elite)
print("Count of elite users:", count_elite)



# Creating another column named 'label'. in which starts column have been converted to numeric
df_user = df_user.withColumn("average_stars", regexp_replace("average_stars", "[^0-9.]+", ""))
df_user = df_user.withColumn("label", df_user["average_stars"].cast("double"))
df_user.show()

df_user = df_user.withColumn("StarBucket", when(df_user["label"] > 4, "5 star")
                                              .when(df_user["label"] >3 , "4 star")
                                              .when(df_user["label"] >2 , "3 star")
                                              .when(df_user["label"] >1 , "2 star")
                                              .otherwise("1 star"))

count_1_star = df_user.filter(df_user.StarBucket == "1 star").count()
count_2_star = df_user.filter(df_user.StarBucket == "2 star").count()
count_3_star = df_user.filter(df_user.StarBucket == "3 star").count()
count_4_star = df_user.filter(df_user.StarBucket == "4 star").count()
count_5_star = df_user.filter(df_user.StarBucket == "5 star").count()

print("Count of 1-star users:", count_1_star)
print("Count of 2-star users:", count_2_star)
print("Count of 3-star users:", count_3_star)
print("Count of 4-star users:", count_4_star)
print("Count of 5-star users:", count_5_star)

# Create a bar plot
labels = ['1-star', '2-star', '3-star', '4-star', '5-star']
counts = [count_1_star, count_2_star, count_3_star, count_4_star, count_5_star]
colors = ['#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']

ax = plt.axes()
ax.bar(labels, counts, color=colors)

# Add labels to the bars
for i, count in enumerate(counts):
    ax.text(i, count+1, str(count), ha='center', va='bottom')

# Add some labels and a title
plt.xlabel('Rating')
plt.ylabel('Count')
plt.title('Ratings Bar Chart')

# Display the plot
plt.show()